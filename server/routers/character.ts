import { router } from "../_core/trpc";
import { studentProcedure, publicProcedure } from "./_procedures";
import { z } from "zod";
import * as db from "../db";
import { TRPCError } from "@trpc/server";
import OpenAI from "openai";

export const characterRouter = router({
  // Get all character types (master data)
  getAllTypes: publicProcedure.query(async () => {
    return await db.getAllCharacterTypes();
  }),

  // Get student's characters
  getMyCharacters: studentProcedure.query(async ({ ctx }) => {
    const student = await db.getStudentByUserId(ctx.user.id);
    if (!student) return [];
    
    return await db.getCharactersByStudentId(student.id);
  }),

  // Create new character
  create: studentProcedure
    .input(z.object({
      name: z.string(),
      animalType: z.string(),
      imageUrl: z.string(),
    }))
    .mutation(async ({ ctx, input }) => {
      const student = await db.getStudentByUserId(ctx.user.id);
      if (!student) {
        throw new TRPCError({ code: 'NOT_FOUND', message: 'Student not found' });
      }
      
      await db.createCharacter({
        studentId: student.id,
        name: input.name,
        animalType: input.animalType,
        imageUrl: input.imageUrl,
      });
      
      return { success: true };
    }),

  // Chat with character using GPT-4o
  chat: studentProcedure
    .input(z.object({
      characterId: z.number(),
      message: z.string(),
      conversationHistory: z.array(z.object({
        role: z.enum(['user', 'assistant']),
        content: z.string(),
      })).optional(),
    }))
    .mutation(async ({ ctx, input }) => {
      const student = await db.getStudentByUserId(ctx.user.id);
      if (!student) {
        throw new TRPCError({ code: 'NOT_FOUND', message: 'Student not found' });
      }

      // Check usage limits
      const usageLimits = await db.checkStudentUsageLimits(student.id);
      
      // If blocked (100%), return fixed response
      if (usageLimits.restrictionLevel === 'blocked') {
        const fixedResponses = [
          'ã”ã‚ã‚“ã­ã€ä»Šæ—¥ã¯ã‚‚ã†ãŠã‚„ã™ã¿ã®æ™‚é–“ã ã‚ˆã€‚ã¾ãŸæ˜æ—¥ãŠè©±ã—ã—ã‚ˆã†ã­ï¼ğŸ˜Š',
          'ãŸãã•ã‚“ãŠè©±ã—ã—ã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ï¼ä»Šæ—¥ã¯ã“ã‚Œã§ãŠã‚ã‚Šã«ã—ã‚ˆã†ã­ã€‚ã¾ãŸæ˜æ—¥ï¼âœ¨',
          'ä»Šæ—¥ã¯ã„ã£ã±ã„ãŠå‹‰å¼·ã—ãŸã­ï¼ã‚‚ã†ä¼‘æ†©ã—ã‚ˆã†ã€‚ã¾ãŸæ˜æ—¥ãŒã‚“ã°ã‚ã†ï¼ğŸŒŸ',
        ];
        return {
          response: fixedResponses[Math.floor(Math.random() * fixedResponses.length)],
          usageInfo: {
            restrictionLevel: 'blocked',
            message: 'ä»Šæ—¥ã®ãŠè©±ã—åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã¾ãŸæ˜æ—¥ãŠè©±ã—ã—ã‚ˆã†ã­ï¼',
          },
        };
      }

      const openai = new OpenAI({
        apiKey: process.env.OPENAI_API_KEY,
      });

      try {
        // Build conversation context
        const messages: Array<{ role: 'system' | 'user' | 'assistant'; content: string }> = [
          {
            role: 'system',
            content: `ã‚ãªãŸã¯å­ä¾›å‘ã‘å­¦ç¿’ã‚¢ãƒ—ãƒªã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã§ã™ã€‚
- å­ä¾›ã«å„ªã—ãã€æ¥½ã—ãä¼šè©±ã—ã¦ãã ã•ã„
- ã²ã‚‰ãŒãªã¨ã‚«ã‚¿ã‚«ãƒŠã‚’ä¸­å¿ƒã«ä½¿ã£ã¦ãã ã•ã„
- å­¦ç¿’ã‚’å¿œæ´ã—ã€åŠ±ã¾ã—ã¦ãã ã•ã„
- çŸ­ãã‚ã‹ã‚Šã‚„ã™ã„è¿”ç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„
- çµµæ–‡å­—ã‚’é©åº¦ã«ä½¿ã£ã¦è¦ªã—ã¿ã‚„ã™ãè©±ã—ã¦ãã ã•ã„`,
          },
        ];

        // Add conversation history (keep last 10 messages for context)
        if (input.conversationHistory && input.conversationHistory.length > 0) {
          const recentHistory = input.conversationHistory.slice(-10);
          messages.push(...recentHistory.map(msg => ({
            role: msg.role === 'user' ? 'user' as const : 'assistant' as const,
            content: msg.content,
          })));
        }

        // Add current message
        messages.push({
          role: 'user',
          content: input.message,
        });

        const startTime = Date.now();
        const completion = await openai.chat.completions.create({
          model: 'gpt-4o',
          messages,
          max_tokens: 150,
          temperature: 0.8,
        });
        const responseTime = Date.now() - startTime;

        const responseText = completion.choices[0]?.message?.content || 'ã”ã‚ã‚“ã­ã€ã‚ˆãã‚ã‹ã‚‰ãªã‹ã£ãŸã‚ˆã€‚';

        // Log usage
        const cost = calculateCost(completion.usage?.prompt_tokens || 0, completion.usage?.completion_tokens || 0);
        await db.logOpenAIUsage({
          userId: ctx.user.id,
          endpoint: 'chat.completions',
          model: 'gpt-4o',
          promptTokens: completion.usage?.prompt_tokens || 0,
          completionTokens: completion.usage?.completion_tokens || 0,
          totalTokens: completion.usage?.total_tokens || 0,
          estimatedCost: cost.toFixed(6),
        });

        return { 
          response: responseText,
          usageInfo: {
            restrictionLevel: usageLimits.restrictionLevel,
            message: usageLimits.restrictionLevel === 'warning' 
              ? 'ãŠè©±ã—ãŒãŸãã•ã‚“ã«ãªã£ã¦ããŸã­ï¼ãã‚ãã‚ä¼‘æ†©ã—ã‚ˆã†ã‹ãªï¼Ÿ'
              : usageLimits.restrictionLevel === 'delay'
              ? 'ãŸãã•ã‚“ãŠè©±ã—ã—ã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ï¼ã‚‚ã†å°‘ã—ã§ä»Šæ—¥ã®ãŠã‚„ã™ã¿æ™‚é–“ã ã‚ˆã€‚'
              : undefined,
          },
        };
      } catch (error) {
        console.error('OpenAI API error:', error);
        // Fallback to simple response
        const responses = [
          'ã“ã‚“ã«ã¡ã¯ï¼ä»Šæ—¥ã‚‚ãŒã‚“ã°ã‚ã†ã­ï¼',
          'ã‚ã‚ï¼ãŸã®ã—ãã†ã ã­ï¼',
          'ã„ã£ã—ã‚‡ã«ãŒã‚“ã°ã‚ã†ï¼',
          'ã™ã”ã„ã­ï¼ã‚‚ã£ã¨ãŠã—ãˆã¦ï¼',
        ];
        return { 
          response: responses[Math.floor(Math.random() * responses.length)]
        };
      }
    }),
});

// Calculate cost for GPT-4o
// Input: $2.50 per 1M tokens
// Output: $10.00 per 1M tokens
function calculateCost(promptTokens: number, completionTokens: number): number {
  const inputCost = (promptTokens / 1_000_000) * 2.5;
  const outputCost = (completionTokens / 1_000_000) * 10.0;
  return inputCost + outputCost;
}
